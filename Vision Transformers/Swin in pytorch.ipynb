{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swin Transformer\n",
    "\n",
    "A hierarchical Transformer whose representation is computed with shifted windows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification and dense prediction tasks such as object detection and semantic segmentation, demonstrating the potential of Transformer-based models as vision backbones.\n",
    "\n",
    "Swim Implementation is from https://github.com/berniwal/swin-transformer-pytorch.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "spjdlxHVYOWf"
   },
   "source": [
    "![](./Sources/2.png)\n",
    "![](./Sources/swin_transformer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_NQHZgBoYQWE",
    "outputId": "e8104382-5c4e-418e-c01b-398196a65f24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Sep  4 13:27:37 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.63.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   35C    P8    28W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I82djvAvY3tY",
    "outputId": "bac84a23-4753-43da-b2a0-720bf0d03a87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: einops==0.3.0 in /usr/local/lib/python3.7/dist-packages (0.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install einops==0.3.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-04T05:53:52.892031Z",
     "start_time": "2021-09-04T05:53:52.590835Z"
    },
    "id": "uiYakZsQYOWl"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, einsum\n",
    "import numpy as np\n",
    "from einops import rearrange, repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swin Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "JlUudv15YOWy"
   },
   "outputs": [],
   "source": [
    "class CyclicShift(nn.Module):\n",
    "    def __init__(self, displacement):\n",
    "        super().__init__()\n",
    "        self.displacement = displacement\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.roll(x, shifts=(self.displacement, self.displacement), dims=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "KlNBz3hvYOW2"
   },
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(x, **kwargs) + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "yVLD15ZwYOW7"
   },
   "outputs": [],
   "source": [
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "8igFetHcYOXC"
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "sF77074-YOXK"
   },
   "outputs": [],
   "source": [
    "def create_mask(window_size, displacement, upper_lower, left_right):\n",
    "    mask = torch.zeros(window_size ** 2, window_size ** 2)\n",
    "\n",
    "    if upper_lower:\n",
    "        mask[-displacement * window_size:, :-displacement * window_size] = float('-inf')\n",
    "        mask[:-displacement * window_size, -displacement * window_size:] = float('-inf')\n",
    "\n",
    "    if left_right:\n",
    "        mask = rearrange(mask, '(h1 w1) (h2 w2) -> h1 w1 h2 w2', h1=window_size, h2=window_size)\n",
    "        mask[:, -displacement:, :, :-displacement] = float('-inf')\n",
    "        mask[:, :-displacement, :, -displacement:] = float('-inf')\n",
    "        mask = rearrange(mask, 'h1 w1 h2 w2 -> (h1 w1) (h2 w2)')\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "jj45SpThZs0S"
   },
   "outputs": [],
   "source": [
    "def get_relative_distances(window_size):\n",
    "    indices = torch.tensor(np.array([[x, y] for x in range(window_size) for y in range(window_size)]))\n",
    "    distances = indices[None, :, :] - indices[:, None, :]\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ZKO693W1Zs6v"
   },
   "outputs": [],
   "source": [
    "class WindowAttention(nn.Module):\n",
    "    def __init__(self, dim, heads, head_dim, shifted, window_size, relative_pos_embedding):\n",
    "        super().__init__()\n",
    "        inner_dim = head_dim * heads\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "        self.window_size = window_size\n",
    "        self.relative_pos_embedding = relative_pos_embedding\n",
    "        self.shifted = shifted\n",
    "\n",
    "        if self.shifted:\n",
    "            displacement = window_size // 2\n",
    "            self.cyclic_shift = CyclicShift(-displacement)\n",
    "            self.cyclic_back_shift = CyclicShift(displacement)\n",
    "            self.upper_lower_mask = nn.Parameter(create_mask(window_size=window_size, displacement=displacement,\n",
    "                                                             upper_lower=True, left_right=False), requires_grad=False)\n",
    "            self.left_right_mask = nn.Parameter(create_mask(window_size=window_size, displacement=displacement,\n",
    "                                                            upper_lower=False, left_right=True), requires_grad=False)\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
    "\n",
    "        if self.relative_pos_embedding:\n",
    "            self.relative_indices = get_relative_distances(window_size) + window_size - 1\n",
    "            self.pos_embedding = nn.Parameter(torch.randn(2 * window_size - 1, 2 * window_size - 1))\n",
    "        else:\n",
    "            self.pos_embedding = nn.Parameter(torch.randn(window_size ** 2, window_size ** 2))\n",
    "\n",
    "        self.to_out = nn.Linear(inner_dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.shifted:\n",
    "            x = self.cyclic_shift(x)\n",
    "\n",
    "        b, n_h, n_w, _, h = *x.shape, self.heads\n",
    "\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        nw_h = n_h // self.window_size\n",
    "        nw_w = n_w // self.window_size\n",
    "\n",
    "        q, k, v = map(\n",
    "            lambda t: rearrange(t, 'b (nw_h w_h) (nw_w w_w) (h d) -> b h (nw_h nw_w) (w_h w_w) d',\n",
    "                                h=h, w_h=self.window_size, w_w=self.window_size), qkv)\n",
    "\n",
    "        dots = einsum('b h w i d, b h w j d -> b h w i j', q, k) * self.scale\n",
    "\n",
    "        if self.relative_pos_embedding:\n",
    "            dots += self.pos_embedding[self.relative_indices[:, :, 0], self.relative_indices[:, :, 1]]\n",
    "        else:\n",
    "            dots += self.pos_embedding\n",
    "\n",
    "        if self.shifted:\n",
    "            dots[:, :, -nw_w:] += self.upper_lower_mask\n",
    "            dots[:, :, nw_w - 1::nw_w] += self.left_right_mask\n",
    "\n",
    "        attn = dots.softmax(dim=-1)\n",
    "\n",
    "        out = einsum('b h w i j, b h w j d -> b h w i d', attn, v)\n",
    "        out = rearrange(out, 'b h (nw_h nw_w) (w_h w_w) d -> b (nw_h w_h) (nw_w w_w) (h d)',\n",
    "                        h=h, w_h=self.window_size, w_w=self.window_size, nw_h=nw_h, nw_w=nw_w)\n",
    "        out = self.to_out(out)\n",
    "\n",
    "        if self.shifted:\n",
    "            out = self.cyclic_back_shift(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "94C9ZopiZtB3"
   },
   "outputs": [],
   "source": [
    "class SwinBlock(nn.Module):\n",
    "    def __init__(self, dim, heads, head_dim, mlp_dim, shifted, window_size, relative_pos_embedding):\n",
    "        super().__init__()\n",
    "        self.attention_block = Residual(PreNorm(dim, WindowAttention(dim=dim,\n",
    "                                                                     heads=heads,\n",
    "                                                                     head_dim=head_dim,\n",
    "                                                                     shifted=shifted,\n",
    "                                                                     window_size=window_size,\n",
    "                                                                     relative_pos_embedding=relative_pos_embedding)))\n",
    "        self.mlp_block = Residual(PreNorm(dim, FeedForward(dim=dim, hidden_dim=mlp_dim)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.attention_block(x)\n",
    "        x = self.mlp_block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "N5iseoebZtHI"
   },
   "outputs": [],
   "source": [
    "class PatchMerging(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, downscaling_factor):\n",
    "        super().__init__()\n",
    "        self.downscaling_factor = downscaling_factor\n",
    "        self.patch_merge = nn.Unfold(kernel_size=downscaling_factor, stride=downscaling_factor, padding=0)\n",
    "        self.linear = nn.Linear(in_channels * downscaling_factor ** 2, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        new_h, new_w = h // self.downscaling_factor, w // self.downscaling_factor\n",
    "        x = self.patch_merge(x).view(b, -1, new_h, new_w).permute(0, 2, 3, 1)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "rbhlzdYcaGWr"
   },
   "outputs": [],
   "source": [
    "class StageModule(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_dimension, layers, downscaling_factor, num_heads, head_dim, window_size,\n",
    "                 relative_pos_embedding):\n",
    "        super().__init__()\n",
    "        assert layers % 2 == 0, 'Stage layers need to be divisible by 2 for regular and shifted block.'\n",
    "\n",
    "        self.patch_partition = PatchMerging(in_channels=in_channels, out_channels=hidden_dimension,\n",
    "                                            downscaling_factor=downscaling_factor)\n",
    "\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(layers // 2):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                SwinBlock(dim=hidden_dimension, heads=num_heads, head_dim=head_dim, mlp_dim=hidden_dimension * 4,\n",
    "                          shifted=False, window_size=window_size, relative_pos_embedding=relative_pos_embedding),\n",
    "                SwinBlock(dim=hidden_dimension, heads=num_heads, head_dim=head_dim, mlp_dim=hidden_dimension * 4,\n",
    "                          shifted=True, window_size=window_size, relative_pos_embedding=relative_pos_embedding),\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_partition(x)\n",
    "        for regular_block, shifted_block in self.layers:\n",
    "            x = regular_block(x)\n",
    "            x = shifted_block(x)\n",
    "        return x.permute(0, 3, 1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tsPokq8JaNfB"
   },
   "source": [
    "# SwinTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "EyRGRkwLaGaA"
   },
   "outputs": [],
   "source": [
    "class SwinTransformer(nn.Module):\n",
    "    def __init__(self, *, hidden_dim, layers, heads, channels=3, num_classes=1000, head_dim=32, window_size=7,\n",
    "                 downscaling_factors=(4, 2, 2, 2), relative_pos_embedding=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.stage1 = StageModule(in_channels=channels, hidden_dimension=hidden_dim, layers=layers[0],\n",
    "                                  downscaling_factor=downscaling_factors[0], num_heads=heads[0], head_dim=head_dim,\n",
    "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
    "        self.stage2 = StageModule(in_channels=hidden_dim, hidden_dimension=hidden_dim * 2, layers=layers[1],\n",
    "                                  downscaling_factor=downscaling_factors[1], num_heads=heads[1], head_dim=head_dim,\n",
    "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
    "        self.stage3 = StageModule(in_channels=hidden_dim * 2, hidden_dimension=hidden_dim * 4, layers=layers[2],\n",
    "                                  downscaling_factor=downscaling_factors[2], num_heads=heads[2], head_dim=head_dim,\n",
    "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
    "        self.stage4 = StageModule(in_channels=hidden_dim * 4, hidden_dimension=hidden_dim * 8, layers=layers[3],\n",
    "                                  downscaling_factor=downscaling_factors[3], num_heads=heads[3], head_dim=head_dim,\n",
    "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_dim * 8),\n",
    "            nn.Linear(hidden_dim * 8, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.stage1(img)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.stage4(x)\n",
    "        x = x.mean(dim=[2, 3])\n",
    "\n",
    "        return self.mlp_head(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swin Models:\n",
    "![](./Sources/4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "RgPhkQmTaGdF"
   },
   "outputs": [],
   "source": [
    "def swin_t(hidden_dim=96, layers=(2, 2, 6, 2), heads=(3, 6, 12, 24), **kwargs):\n",
    "    return SwinTransformer(hidden_dim=hidden_dim, layers=layers, heads=heads, **kwargs)\n",
    "\n",
    "\n",
    "def swin_s(hidden_dim=96, layers=(2, 2, 18, 2), heads=(3, 6, 12, 24), **kwargs):\n",
    "    return SwinTransformer(hidden_dim=hidden_dim, layers=layers, heads=heads, **kwargs)\n",
    "\n",
    "\n",
    "def swin_b(hidden_dim=128, layers=(2, 2, 18, 2), heads=(4, 8, 16, 32), **kwargs):\n",
    "    return SwinTransformer(hidden_dim=hidden_dim, layers=layers, heads=heads, **kwargs)\n",
    "\n",
    "\n",
    "def swin_l(hidden_dim=192, layers=(2, 2, 18, 2), heads=(6, 12, 24, 48), **kwargs):\n",
    "    return SwinTransformer(hidden_dim=hidden_dim, layers=layers, heads=heads, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cheg50xIYOXP",
    "outputId": "fb230d2a-8f35-4a30-cab6-a61b6f4f8eba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SwinTransformer(\n",
      "  (stage1): StageModule(\n",
      "    (patch_partition): PatchMerging(\n",
      "      (patch_merge): Unfold(kernel_size=4, dilation=1, padding=0, stride=4)\n",
      "      (linear): Linear(in_features=48, out_features=96, bias=True)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): ModuleList(\n",
      "        (0): SwinBlock(\n",
      "          (attention_block): Residual(\n",
      "            (fn): PreNorm(\n",
      "              (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): WindowAttention(\n",
      "                (to_qkv): Linear(in_features=96, out_features=288, bias=False)\n",
      "                (to_out): Linear(in_features=96, out_features=96, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (mlp_block): Residual(\n",
      "            (fn): PreNorm(\n",
      "              (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): FeedForward(\n",
      "                (net): Sequential(\n",
      "                  (0): Linear(in_features=96, out_features=384, bias=True)\n",
      "                  (1): GELU()\n",
      "                  (2): Linear(in_features=384, out_features=96, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): SwinBlock(\n",
      "          (attention_block): Residual(\n",
      "            (fn): PreNorm(\n",
      "              (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): WindowAttention(\n",
      "                (cyclic_shift): CyclicShift()\n",
      "                (cyclic_back_shift): CyclicShift()\n",
      "                (to_qkv): Linear(in_features=96, out_features=288, bias=False)\n",
      "                (to_out): Linear(in_features=96, out_features=96, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (mlp_block): Residual(\n",
      "            (fn): PreNorm(\n",
      "              (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): FeedForward(\n",
      "                (net): Sequential(\n",
      "                  (0): Linear(in_features=96, out_features=384, bias=True)\n",
      "                  (1): GELU()\n",
      "                  (2): Linear(in_features=384, out_features=96, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (stage2): StageModule(\n",
      "    (patch_partition): PatchMerging(\n",
      "      (patch_merge): Unfold(kernel_size=2, dilation=1, padding=0, stride=2)\n",
      "      (linear): Linear(in_features=384, out_features=192, bias=True)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): ModuleList(\n",
      "        (0): SwinBlock(\n",
      "          (attention_block): Residual(\n",
      "            (fn): PreNorm(\n",
      "              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): WindowAttention(\n",
      "                (to_qkv): Linear(in_features=192, out_features=576, bias=False)\n",
      "                (to_out): Linear(in_features=192, out_features=192, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (mlp_block): Residual(\n",
      "            (fn): PreNorm(\n",
      "              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): FeedForward(\n",
      "                (net): Sequential(\n",
      "                  (0): Linear(in_features=192, out_features=768, bias=True)\n",
      "                  (1): GELU()\n",
      "                  (2): Linear(in_features=768, out_features=192, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): SwinBlock(\n",
      "          (attention_block): Residual(\n",
      "            (fn): PreNorm(\n",
      "              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): WindowAttention(\n",
      "                (cyclic_shift): CyclicShift()\n",
      "                (cyclic_back_shift): CyclicShift()\n",
      "                (to_qkv): Linear(in_features=192, out_features=576, bias=False)\n",
      "                (to_out): Linear(in_features=192, out_features=192, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (mlp_block): Residual(\n",
      "            (fn): PreNorm(\n",
      "              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): FeedForward(\n",
      "                (net): Sequential(\n",
      "                  (0): Linear(in_features=192, out_features=768, bias=True)\n",
      "                  (1): GELU()\n",
      "                  (2): Linear(in_features=768, out_features=192, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (stage3): StageModule(\n",
      "    (patch_partition): PatchMerging(\n",
      "      (patch_merge): Unfold(kernel_size=2, dilation=1, padding=0, stride=2)\n",
      "      (linear): Linear(in_features=768, out_features=384, bias=True)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): ModuleList(\n",
      "        (0): SwinBlock(\n",
      "          (attention_block): Residual(\n",
      "            (fn): PreNorm(\n",
      "              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): WindowAttention(\n",
      "                (to_qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
      "                (to_out): Linear(in_features=384, out_features=384, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (mlp_block): Residual(\n",
      "            (fn): PreNorm(\n",
      "              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): FeedForward(\n",
      "                (net): Sequential(\n",
      "                  (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                  (1): GELU()\n",
      "                  (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): SwinBlock(\n",
      "          (attention_block): Residual(\n",
      "            (fn): PreNorm(\n",
      "              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): WindowAttention(\n",
      "                (cyclic_shift): CyclicShift()\n",
      "                (cyclic_back_shift): CyclicShift()\n",
      "                (to_qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
      "                (to_out): Linear(in_features=384, out_features=384, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (mlp_block): Residual(\n",
      "            (fn): PreNorm(\n",
      "              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): FeedForward(\n",
      "                (net): Sequential(\n",
      "                  (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                  (1): GELU()\n",
      "                  (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): ModuleList(\n",
      "        (0): SwinBlock(\n",
      "          (attention_block): Residual(\n",
      "            (fn): PreNorm(\n",
      "              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): WindowAttention(\n",
      "                (to_qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
      "                (to_out): Linear(in_features=384, out_features=384, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (mlp_block): Residual(\n",
      "            (fn): PreNorm(\n",
      "              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): FeedForward(\n",
      "                (net): Sequential(\n",
      "                  (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                  (1): GELU()\n",
      "                  (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): SwinBlock(\n",
      "          (attention_block): Residual(\n",
      "            (fn): PreNorm(\n",
      "              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): WindowAttention(\n",
      "                (cyclic_shift): CyclicShift()\n",
      "                (cyclic_back_shift): CyclicShift()\n",
      "                (to_qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
      "                (to_out): Linear(in_features=384, out_features=384, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (mlp_block): Residual(\n",
      "            (fn): PreNorm(\n",
      "              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): FeedForward(\n",
      "                (net): Sequential(\n",
      "                  (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                  (1): GELU()\n",
      "                  (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): ModuleList(\n",
      "        (0): SwinBlock(\n",
      "          (attention_block): Residual(\n",
      "            (fn): PreNorm(\n",
      "              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): WindowAttention(\n",
      "                (to_qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
      "                (to_out): Linear(in_features=384, out_features=384, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (mlp_block): Residual(\n",
      "            (fn): PreNorm(\n",
      "              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): FeedForward(\n",
      "                (net): Sequential(\n",
      "                  (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                  (1): GELU()\n",
      "                  (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): SwinBlock(\n",
      "          (attention_block): Residual(\n",
      "            (fn): PreNorm(\n",
      "              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): WindowAttention(\n",
      "                (cyclic_shift): CyclicShift()\n",
      "                (cyclic_back_shift): CyclicShift()\n",
      "                (to_qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
      "                (to_out): Linear(in_features=384, out_features=384, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (mlp_block): Residual(\n",
      "            (fn): PreNorm(\n",
      "              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): FeedForward(\n",
      "                (net): Sequential(\n",
      "                  (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                  (1): GELU()\n",
      "                  (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (stage4): StageModule(\n",
      "    (patch_partition): PatchMerging(\n",
      "      (patch_merge): Unfold(kernel_size=2, dilation=1, padding=0, stride=2)\n",
      "      (linear): Linear(in_features=1536, out_features=768, bias=True)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): ModuleList(\n",
      "        (0): SwinBlock(\n",
      "          (attention_block): Residual(\n",
      "            (fn): PreNorm(\n",
      "              (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): WindowAttention(\n",
      "                (to_qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
      "                (to_out): Linear(in_features=768, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (mlp_block): Residual(\n",
      "            (fn): PreNorm(\n",
      "              (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): FeedForward(\n",
      "                (net): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (1): GELU()\n",
      "                  (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): SwinBlock(\n",
      "          (attention_block): Residual(\n",
      "            (fn): PreNorm(\n",
      "              (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): WindowAttention(\n",
      "                (cyclic_shift): CyclicShift()\n",
      "                (cyclic_back_shift): CyclicShift()\n",
      "                (to_qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
      "                (to_out): Linear(in_features=768, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (mlp_block): Residual(\n",
      "            (fn): PreNorm(\n",
      "              (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): FeedForward(\n",
      "                (net): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (1): GELU()\n",
      "                  (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (mlp_head): Sequential(\n",
      "    (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): Linear(in_features=768, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "net = SwinTransformer(\n",
    "    hidden_dim=96,\n",
    "    layers=(2, 2, 6, 2),\n",
    "    heads=(3, 6, 12, 24),\n",
    "    channels=3,\n",
    "    num_classes=3,\n",
    "    head_dim=32,\n",
    "    window_size=7,\n",
    "    downscaling_factors=(4, 2, 2, 2),\n",
    "    relative_pos_embedding=True\n",
    ")\n",
    "dummy_x = torch.randn(1, 3, 224, 224)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HbkvCLAt_iPq",
    "outputId": "03f1f872-083e-4234-9d54-3d0af42fb34a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3377,  0.8965,  0.5093]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "logits = net(dummy_x)  # (1,3)\n",
    "print(logits)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Swin Transformer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
